{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6132195,"sourceType":"datasetVersion","datasetId":3466975}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport torch.optim as optim","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-22T02:40:26.698177Z","iopub.execute_input":"2024-08-22T02:40:26.698577Z","iopub.status.idle":"2024-08-22T02:40:34.946957Z","shell.execute_reply.started":"2024-08-22T02:40:26.698544Z","shell.execute_reply":"2024-08-22T02:40:34.945797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/vietnamese-online-news-csv-dataset/Fixed_news_dataset.csv\")\ndf = df[['content', 'title', 'topic']]\ndf.dropna(subset=['content', 'title', 'topic'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-22T02:40:34.949153Z","iopub.execute_input":"2024-08-22T02:40:34.949849Z","iopub.status.idle":"2024-08-22T02:42:13.632292Z","shell.execute_reply.started":"2024-08-22T02:40:34.949781Z","shell.execute_reply":"2024-08-22T02:42:13.631185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        # tokenization\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n\n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'labels': torch.tensor(label, dtype=torch.long)\n        }\n\n# combine title and content\ndf['text'] = df['title'] + \" \" + df['content']\n\n# encode topics\nlabel_encoder = LabelEncoder()\ndf['label'] = label_encoder.fit_transform(df['topic'])\n\n# Train-test split\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df['text'].values,\n    df['label'].values,\n    test_size=0.2,\n    random_state=42,\n)\n\n# initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmax_len = 512\n\n# create datasets and dataLoaders\ntrain_dataset = NewsDataset(train_texts, train_labels, tokenizer, max_len)\nval_dataset = NewsDataset(val_texts, val_labels, tokenizer, max_len)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T02:48:01.541739Z","iopub.execute_input":"2024-08-21T02:48:01.542126Z","iopub.status.idle":"2024-08-21T02:48:05.266959Z","shell.execute_reply.started":"2024-08-21T02:48:01.542093Z","shell.execute_reply":"2024-08-21T02:48:05.264755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T02:51:29.299061Z","iopub.execute_input":"2024-08-21T02:51:29.300108Z","iopub.status.idle":"2024-08-21T02:51:29.307092Z","shell.execute_reply.started":"2024-08-21T02:51:29.300067Z","shell.execute_reply":"2024-08-21T02:51:29.305727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hybrid CNN-LSTM","metadata":{}},{"cell_type":"code","source":"class HybridCNNLSTM(nn.Module):\n    def __init__(self, vocab_size, embed_size, num_classes, cnn_out_channels=128, lstm_hidden_size=128, lstm_layers=1, kernel_size=3, dropout=0.5):\n        super(HybridCNNLSTM, self).__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.conv = nn.Conv1d(in_channels=embed_size, out_channels=cnn_out_channels, kernel_size=kernel_size, padding=1)\n        self.relu = nn.ReLU()\n        self.lstm = nn.LSTM(input_size=cnn_out_channels, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True, bidirectional=True)\n        self.fc = nn.Linear(lstm_hidden_size * 2, num_classes)  # *2 for bidirectional\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        # Embedding\n        x = self.embedding(x)\n        x = x.permute(0, 2, 1)  # (batch_size, embed_size, sequence_length)\n        \n        # cNN\n        x = self.conv(x)\n        x = self.relu(x)\n        x = x.permute(0, 2, 1)  # (batch_size, sequence_length, cnn_out_channels)\n        \n        # lSTM\n        x, _ = self.lstm(x)\n        \n        # max pooling\n        x, _ = torch.max(x, dim=1)\n        \n        # fully connected layer with dropout\n        x = self.dropout(x)\n        x = self.fc(x)\n        \n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T02:49:43.280086Z","iopub.execute_input":"2024-08-21T02:49:43.280605Z","iopub.status.idle":"2024-08-21T02:49:43.296146Z","shell.execute_reply.started":"2024-08-21T02:49:43.280537Z","shell.execute_reply":"2024-08-21T02:49:43.294639Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(tokenizer.vocab) \nembed_size = 128                   # embed size\nnum_classes = len(label_encoder.classes_)  # Number of output classes\n\n# instantiate model\nmodel = HybridCNNLSTM(vocab_size, embed_size, num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T03:05:24.015203Z","iopub.execute_input":"2024-08-21T03:05:24.016184Z","iopub.status.idle":"2024-08-21T03:05:25.293239Z","shell.execute_reply.started":"2024-08-21T03:05:24.015924Z","shell.execute_reply":"2024-08-21T03:05:25.291652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 10\n\nfor epoch in range(num_epochs):\n    model.train()\n    train_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n    \n    for batch in train_loader:\n        input_ids = batch['input_ids'].to(device)\n        labels = batch['labels'].to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(input_ids)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, preds = torch.max(outputs, dim=1)\n        correct_predictions += torch.sum(preds == labels)\n        total_predictions += labels.size(0)\n    \n    avg_train_loss = train_loss / len(train_loader)\n    train_accuracy = correct_predictions.double() / total_predictions\n    \n    train_losses.append(avg_train_loss)\n    train_accuracies.append(train_accuracy.item())\n\n    # valid\n    model.eval()\n    val_loss = 0\n    correct_predictions = 0\n    total_predictions = 0\n    \n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            labels = batch['labels'].to(device)\n            \n            outputs = model(input_ids)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, preds = torch.max(outputs, dim=1)\n            correct_predictions += torch.sum(preds == labels)\n            total_predictions += labels.size(0)\n    \n    avg_val_loss = val_loss / len(val_loader)\n    val_accuracy = correct_predictions.double() / total_predictions\n    \n    val_losses.append(avg_val_loss)\n    val_accuracies.append(val_accuracy.item())\n    \n    print(f\"Epoch [{epoch + 1}/{num_epochs}], \"\n          f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n          f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T03:07:39.742407Z","iopub.execute_input":"2024-08-21T03:07:39.743372Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loss\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Train Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Over Epochs')\nplt.legend()\n\n# acc\nplt.subplot(1, 2, 2)\nplt.plot(train_accuracies, label='Train Accuracy')\nplt.plot(val_accuracies, label='Validation Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Accuracy Over Epochs')\nplt.legend()\n\nplt.show()\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}